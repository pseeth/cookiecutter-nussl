---
# The sweep config is used to replace dictionary keys below and create experiments
# on the fly. A separate experiment will be created for each sweep discovered. The
# set of experiments can then be submitted to the job runner in parallel or in sequence.
# If one of the arguments is a list, then it will loop across each of the items in the
# list creating a separate experiment for each one. The sweep configuration below 
# creates 10 experiments in total. This sweep config is very simple, so be very careful
# when setting things up as creating invalid or buggy experiments (e.g. num_frequencies
# and n_fft don't match) is possible. If there's no sweep config, then only one 
# experiment is created with whatever is in the rest of the file. Each sweep within 
# an item of the list should use the same cache. The cache will be created as a separate
# experiment. 

# The config below creates one cache with STFTs with n_fft=128 and hop_length=64. 
# Two experiments are created per sweeping across the number of hidden units. Since
# the number of hidden units in RecurrentStack needs to match the number of hidden_size
# in the Embedding module, the "multiple parameters" option is used.

# If there is a '.' in the key, then it is an absolute path to the exact value to update
# in the configuration. If there isn't, then it is a global update for all matching keys.
sweep:
  - n_fft: 512
    hop_length: 128
    num_frequencies: 257 # n_fft / 2 + 1
    num_features: 257
    num_epochs: 100
    model_config.modules.recurrent_stack.args.num_layers: 4
    cache: '${CACHE_DIRECTORY}/musdb_128'
    populate_cache: true # controls whether to create a separate experiment for caching
    num_cache_workers: 60 # how many workers to use when populating the cache
    # multiple parameters can be sweeped across in pairs if 
    # there are dependencies between them. use 'multiple_parameters' in the key
    multiple_parameters_1: # maybe make this a list of tuples? dictionary of configs?
      - bidirectional: true
        model_config.modules.recurrent_stack.args.hidden_size: 300
        num_mels: -1
        model_config.modules.embedding.args.hidden_size: 600
      - bidirectional: true
        model_config.modules.recurrent_stack.args.hidden_size: 150
        num_mels: -1
        model_config.modules.embedding.args.hidden_size: 300

info:
  blocking: false
  num_gpus: 1
  project_name: cookiecutter/music
  spreadsheet_name: nussl-testbed
  worksheet_name: musdb_samples
  output_folder: ${ARTIFACTS_DIRECTORY}/cookiecutter/music/${EXPERIMENT_KEY}
  experiment_key: ${EXPERIMENT_KEY}

dataset_config:
  cache: '${CACHE_DIRECTORY}/musdb' # Path is relative to environment variable CACHE_DIRECTORY
  data_keys_for_training:
  - log_spectrogram
  - assignments
  - weights
  excerpt_selection_strategy: random
  format: rnn
  fraction_of_dataset: 1.0
  group_sources: 
  - [drums, bass, other]
  ignore_sources: []
  length: 400
  n_fft: 512
  hop_length: 128
  num_channels: 1
  output_type: psa
  overwrite_cache: false
  sample_rate: 16000
  source_labels: []
  use_librosa_stft: false
  weight_threshold: -80
  weight_type:
  - magnitude

datasets:
  # Three subsets - train/test/val
  # Each subset has a folder and a class.
  # Paths are relative to the environment variable DATA_DIRECTORY
  #   see setup/environment/default.sh for details
  train:
    folder: ${DATA_DIRECTORY}/musdb18_sample/generated/coherent/train/
    class: Scaper
  val:
    folder: ${DATA_DIRECTORY}/musdb18_sample/generated/coherent/val/
    class: Scaper
  test:
    folder: ${DATA_DIRECTORY}/musdb18_sample/generated/coherent/test/
    class: Scaper

train_config:
  class: Trainer
  batch_size: 40
  curriculum_learning:
  - args: [400]
    command: set_current_length
    num_epoch: 0
  data_parallel: true
  device: cuda
  initial_length: 400
  learning_rate: 0.0002
  learning_rate_decay: 0.5
  loss_function:
  - !!python/tuple
    - dpcl            # name of loss function
    - embedding       # what output of model to apply the loss function on
    - 1.0             # weight given to the loss function
  num_epochs: 10
  num_workers: 20
  optimizer: adam
  patience: 5
  sample_strategy: sequential
  weight_decay: 0.0

model_config:
  class: SeparationModel
  modules:
    log_spectrogram:
      input_shape: [-1, -1, 257]
    normalization:
      class: InstanceNorm
      args:
        use_instance_norm: true
    mel_projection:
      class: MelProjection
      args:
        sample_rate: 16000
        num_frequencies: 257
        num_mels: -1
        direction: forward
        trainable: false
        clamp: false
    recurrent_stack:
      class: RecurrentStack
      args:
        num_features: 257
        hidden_size: 10
        num_layers: 2
        bidirectional: true
        dropout: 0.3
        rnn_type: lstm
    embedding:
      class: Embedding
      args:
        num_features: 257
        num_channels: 1
        hidden_size: 20
        embedding_size: 20
        activation:
        - tanh

  connections:
    - !!python/tuple      # tuple containing two things:
      - mel_projection    # unique name given to module above
      - [log_spectrogram] # list of runtime arguments needed by that module 
                          # (e.g. output of prev layer)
    - !!python/tuple
      - normalization
      - [mel_projection]
    - !!python/tuple
      - recurrent_stack
      - [normalization]
    - !!python/tuple
      - embedding
      - [recurrent_stack]

  output:
  - embedding

algorithm_config:
  class: DeepClustering
  args:
    clustering_options:
      posterior_alpha: 5.0
    enhancement_amount: 0.0
    mask_type: soft
    model_path: ${ARTIFACTS_DIRECTORY}/cookiecutter/music/${EXPERIMENT_KEY}/checkpoints/best.model.pth
    num_sources: 2
    percentile: 95

test_config:
  num_workers: 10
  use_blocking_executor: true
  seed: 0
  testers:
    ScaleInvariantSDR:
      compute_permutation: true
      scaling: true
      source_labels: [vocals, accompaniment]