---
# The sweep config is used to replace dictionary keys below and create experiments
# on the fly. A separate experiment will be created for each sweep discovered. The
# set of experiments can then be submitted to the job runner in parallel or in sequence.
# If one of the arguments is a list, then it will loop across each of the items in the
# list creating a separate experiment for each one. The sweep configuration below 
# creates 10 experiments in total. This sweep config is very simple, so be very careful
# when setting things up as creating invalid or buggy experiments (e.g. num_frequencies
# and n_fft don't match) is possible. If there's no sweep config, then only one 
# experiment is created with whatever is in the rest of the file. Each sweep within 
# an item of the list should use the same cache. The cache will be created as a separate
# experiment. 

# The config below creates two caches: one with STFTs with n_fft=128 and hop_length=64
# and the other with  n_fft=512, hop_length=128. Four experiments are created per
# cache, sweeping across num_layers (1 or 2), and bidirectionality (true or false).
sweep:
  - n_fft: 128
    hop_length: 64
    num_frequencies: 65
    num_mels: -1
    num_features: 65
    num_layers: [1, 2]
    cache: 'CACHE_DIRECTORY/musdb_128'
    populate_cache: true # controls whether to create a separate experiment for caching
    num_cache_workers: 60 # how many workers to use when populating the cache
    # multiple parameters can be sweeped across in pairs if 
    # there are dependencies between them. use 'multiple_parameters' in the key
    # to indicate this.
    multiple_parameters_1: 
      - bidirectional: true
        hidden_size: 600
      - bidirectional: false
        hidden_size: 300

  - n_fft: 512
    hop_length: 128
    num_frequencies: 257
    num_mels: -1
    num_features: 257
    num_layers: [1, 2]
    cache: 'CACHE_DIRECTORY/musdb_512'
    populate_cache: true # controls whether to create a separate experiment for caching
    num_cache_workers: 60    # how many workers to use when populating the cache
    # multiple parameters can be sweeped across in pairs if 
    # there are dependencies between them. use 'multiple_parameters' in the key
    # to indicate this.
    multiple_parameters_1: 
      - bidirectional: true
        hidden_size: 600
      - bidirectional: false
        hidden_size: 300

# Everything below is static with certain elements being replaced by things in the 
# sweep config above.
dataset_config:
  cache: 'CACHE_DIRECTORY/musdb' # Path is relative to environment variable CACHE_DIRECTORY
  data_keys_for_training:
  - log_spectrogram
  - assignments
  - weights
  excerpt_selection_strategy: random
  format: rnn
  fraction_of_dataset: 1.0
  group_sources: 
  - [drums, bass, other]
  ignore_sources: []
  length: 400
  n_fft: 512
  hop_length: 128
  num_channels: 1
  output_type: psa
  overwrite_cache: false
  sample_rate: 16000
  source_labels: []
  use_librosa_stft: false
  weight_threshold: -80
  weight_type:
  - magnitude

datasets:
  # Three subsets - train/test/val
  # Each subset has a folder and a class.
  # Paths are relative to the environment variable DATA_DIRECTORY
  #   see setup/environment/default.sh for details
  train:
    folder: DATA_DIRECTORY/babymusdb/generated/coherent/train/
    class: Scaper
  val:
    folder: DATA_DIRECTORY/babymusdb/generated/coherent/val/
    class: Scaper
  test:
    folder: DATA_DIRECTORY/babymusdb/generated/coherent/test/
    class: Scaper

info:
  blocking: false
  num_gpus: 1
  project_name: cookiecutter/music
  output_folder: ARTIFACTS_DIRECTORY/cookiecutter/music/EXPERIMENT_KEY
  experiment_key:

train_config:
  class: Trainer
  batch_size: 40
  curriculum_learning:
  - args: [400]
    command: set_current_length
    num_epoch: 0
  data_parallel: true
  device: cuda
  initial_length: 400
  learning_rate: 0.0002
  learning_rate_decay: 0.5
  loss_function:
  - !!python/tuple
    - dpcl            # name of loss function
    - embedding       # what output of model to apply the loss function on
    - 1.0             # weight given to the loss function
  num_epochs: 10
  num_workers: 20
  optimizer: adam
  patience: 5
  sample_strategy: sequential
  weight_decay: 0.0

model_config:
  class: SeparationModel
  modules:
    log_spectrogram:
      input_shape: [-1, -1, 257]
    normalization:
      class: BatchNorm
      args:
        use_batch_norm: true
    mel_projection:
      class: MelProjection
      args:
        sample_rate: 16000
        num_frequencies: 257
        num_mels: -1
        direction: forward
        trainable: false
        clamp: false
    recurrent_stack:
      class: RecurrentStack
      args:
        num_features: 257
        hidden_size: 300
        num_layers: 2
        bidirectional: true
        dropout: 0.3
        rnn_type: lstm
    embedding:
      class: Embedding
      args:
        num_features: 257
        num_channels: 1
        hidden_size: 600
        embedding_size: 20
        activation:
        - tanh

  connections:
    - !!python/tuple      # tuple containing two things:
      - mel_projection    # unique name given to module above
      - [log_spectrogram] # list of runtime arguments needed by that module 
                          # (e.g. output of prev layer)
    - !!python/tuple
      - normalization
      - [mel_projection]
    - !!python/tuple
      - recurrent_stack
      - [normalization]
    - !!python/tuple
      - embedding
      - [recurrent_stack]

  output:
  - embedding

algorithm_config:
  class: DeepClustering
  params:
    clustering_options:
      posterior_alpha: 5.0
    enhancement_amount: 0.0
    mask_type: soft
    model_path: ARTIFACTS_DIRECTORY/cookiecutter/music/EXPERIMENT_KEY/checkpoints/best.model.pth
    num_sources: 2
    percentile: 95

test_config:
  num_workers: 10
  testers:
  - SDR