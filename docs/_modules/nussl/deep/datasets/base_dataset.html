

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nussl.deep.datasets.base_dataset &mdash; Cookiecutter for nussl  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> Cookiecutter for nussl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started.html">Getting started</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../using_scaper.html">Using Scaper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../experiments.html">Configuring an experiment</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/runners.html">runners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/scripts.html">scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/src.html">src</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Cookiecutter for nussl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>nussl.deep.datasets.base_dataset</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nussl.deep.datasets.base_dataset</h1><div class="highlight"><pre>
<span></span>from torch.utils.data import Dataset
import pickle
import librosa
import numpy as np
import os
import shutil
import random
from typing import Dict, Any, Optional, Tuple, List
from ...core import AudioSignal, jupyter_utils
from ...separation import SoftMask
from scipy.io import wavfile
import logging
import copy
from enum import Enum
import zarr
import numcodecs
import matplotlib.pyplot as plt

<div class="viewcode-block" id="BaseDataset"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset">[docs]</a>class BaseDataset(Dataset):
    def __init__(self, folder: str, options: Dict[str, Any]):
        &quot;&quot;&quot;Implements a variety of methods for loading source separation
        datasets such as WSJ0-[2,3]mix and datasets made with Scaper.

        Arguments:
            folder - Folder where dataset is contained.

        Keyword Arguments:
            options - a dictionary containing the settings for the dataset
                loader. See `config/defaults/metadata/dataset.json` for full
                description.
        &quot;&quot;&quot;

        self.folder = folder
        self.options = copy.deepcopy(options)
        self.use_librosa = self.options.pop(&#39;use_librosa_stft&#39;, False)
        self.dataset_tag = self.options.pop(&#39;dataset_tag&#39;, &#39;default&#39;)
        self.current_length = self.options.pop(&#39;min_length&#39;, self.options[&#39;length&#39;])
        self.excerpt_selection_strategy = self.options.pop(&#39;excerpt_selection_strategy&#39;, &#39;random&#39;)
        self.chunk_size = self.options.pop(&#39;chunk_size&#39;, 1)

        self.files = self.get_files(self.folder)
        random.shuffle(self.files)
        self.cached_files = []
        self.targets = [
            &#39;log_spectrogram&#39;,
            &#39;magnitude_spectrogram&#39;,
            &#39;assignments&#39;,
            &#39;source_spectrograms&#39;,
            &#39;weights&#39;
        ]
        self.data_keys_for_training = self.options.pop(&#39;data_keys_for_training&#39;, [])
        self.targets += self.data_keys_for_training
        self.targets = list(set(self.targets))
        self.setup_cache()
        self.cache_populated = False

        if self.options[&#39;fraction_of_dataset&#39;] &lt; 1.0:
            num_files = int(
                len(self.files) * self.options[&#39;fraction_of_dataset&#39;]
            )
            self.files = self.files[:num_files]

<div class="viewcode-block" id="BaseDataset.setup_cache"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.setup_cache">[docs]</a>    def setup_cache(self):
        &quot;&quot;&quot;Items generated by the dataset can be cached using zarr. Zarr is a
        library for easily storing data in efficient and compressed structures. 
        Every time BaseDataset&#39;s __getitem__ method is called, it returns a 
        dictionary with keys and values (usually numpy arrays but they can be 
        anything). If self.options[&#39;cache&#39;] points to some place (is a string of
        nonzero length), then a cache is created. The name of the cache follows 
        a specific format, allowing for cache reuse. It uses the dataset location
        with underscores instead of /, and appends the dataset_tag, kept in 
        self.options[&#39;dataset_tag&#39;], and adds the extension &#39;.zarr&#39;. The location
        will be printed by the logging.
        &quot;&quot;&quot;
        if self.options[&#39;cache&#39;]:
            cache = os.path.join(
                os.path.expanduser(self.options[&#39;cache&#39;]),
                &#39;_&#39;.join(self.folder.split(&#39;/&#39;)),
                self.dataset_tag + &#39;.zarr&#39;
            )
            self.cache = cache
            self.overwrite_cache = self.options.pop(&#39;overwrite_cache&#39;, False)
            logging.info(f&#39;Caching to: {self.cache}&#39;)

            file_mode = &#39;r&#39;
            if os.path.exists(self.cache):
                logging.info(f&#39;Cache location {self.cache} exists! Checking if overwrite. Otherwise, will use cache.&#39;)
                if self.overwrite_cache:
                    logging.info(&#39;Overwriting cache.&#39;)
                    self.clear_cache()
                    file_mode = &#39;w&#39;
            else:
                logging.info(f&#39;{self.cache} does not exist...creating a new cache&#39;)
                self.overwrite_cache = True
                file_mode = &#39;w&#39;
            
            self.cache_dataset = zarr.open(
                self.cache, 
                mode=file_mode, 
                shape=(len(self),), 
                chunks=(self.chunk_size,),
                dtype=object, 
                object_codec=numcodecs.Pickle(),
                synchronizer=zarr.ThreadSynchronizer(),
            )</div>

<div class="viewcode-block" id="BaseDataset.clear_cache"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.clear_cache">[docs]</a>    def clear_cache(self):
        &quot;&quot;&quot;Deletes the cache.
        &quot;&quot;&quot;
        logging.info(f&#39;Clearing cache: {self.cache}&#39;)
        shutil.rmtree(self.cache, ignore_errors=True)</div>
    
<div class="viewcode-block" id="BaseDataset.populate_cache"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.populate_cache">[docs]</a>    def populate_cache(self, filename, i):
        &quot;&quot;&quot;Generates an example and puts it in the cache.
        &quot;&quot;&quot;
        output = self._generate_example(filename)
        if self.data_keys_for_training:
            output = {
                k: output[k] for k in output 
                if k in self.data_keys_for_training
            }
        self.write_to_cache(output, i)
        return self.get_target_length(output, self.current_length)</div>

<div class="viewcode-block" id="BaseDataset.switch_to_cache"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.switch_to_cache">[docs]</a>    def switch_to_cache(self):
        &quot;&quot;&quot;In the first pass through the data, the cache is created on the fly. On
        subsequent epochs, you should use `switch_to_cache` to switch to the cache.
        This function closes and reopens the cache in read-only mode, and sets the 
        internal flag cache_populated to True. Now future reads are done via the
        cache.
        &quot;&quot;&quot;
        self.cache_dataset = zarr.open(
            self.cache, 
            mode=&#39;r&#39;, 
            shape=(len(self.files),), 
            chunks=(self.chunk_size,),
            dtype=object, 
            object_codec=numcodecs.Pickle(),
            synchronizer=zarr.ThreadSynchronizer(),
        )
        self.cache_populated = True</div>

<div class="viewcode-block" id="BaseDataset.write_to_cache"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.write_to_cache">[docs]</a>    def write_to_cache(self, data_dict, i):
        &quot;&quot;&quot;Writes a dictionary of keys and values to a location in the cache.
        &quot;&quot;&quot;
        self.cache_dataset[i] = data_dict</div>

<div class="viewcode-block" id="BaseDataset.load_from_cache"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.load_from_cache">[docs]</a>    def load_from_cache(self, i):
        &quot;&quot;&quot;Reads the ith dictionary from the cache.
        &quot;&quot;&quot;
        data = self.cache_dataset[i]
        return self.get_target_length(data, self.current_length)</div>
            
<div class="viewcode-block" id="BaseDataset.get_files"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.get_files">[docs]</a>    def get_files(self, folder):
        &quot;&quot;&quot;This function must be implemented by whatever class inherits BaseDataset.
        It should return a list of files in the given folder, each of which is 
        processed by load_audio_files in some way to produce mixes, sources, class
        labels, etc.

        Args:
            folder - location that should be processed to produce the list of files.

        Returns:
            list of files
        &quot;&quot;&quot;
        raise NotImplementedError()</div>

<div class="viewcode-block" id="BaseDataset.load_audio_files"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.load_audio_files">[docs]</a>    def load_audio_files(
            self,
            filename: str
        ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:
        &quot;&quot;&quot;Each file returned by get_files is processed by this function. For example,
        if each file is a json file containing the paths to the mixture and sources, 
        then this function should parse the json file and load the mixture and sources
        and return them.

        Exact behavior of this functionality is determined by implementation by subclass.

        Args:
            filename - name of file to load

        Returns:
            Depends on sub-class. Should return at least the mixture and the constituent
            sources.
        &quot;&quot;&quot;
        raise NotImplementedError()</div>

    def __len__(self) -&gt; int:
        &quot;&quot;&quot;Gets number of examples&quot;&quot;&quot;
        return len(self.files)

    def __getitem__(self, i: int) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Gets one item from dataset

        Args:
            i - index of example to get

        Returns:
            one data point (an output dictionary containing the data comprising
            one example)
        &quot;&quot;&quot;
        return self._get_item_helper(self.files[i], self.cache, i)

    def _get_item_helper(
        self,
        filename: str,
        cache: Optional[str],
        i: int = -1,
    ) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Gets one item from dataset

        If `cache` is None, will generate an example (training|validation) from
        scratch. If `cache` is not None, it will attempt to read from the path
        given by `cache`. On failure it will write to the path given by `cache`
        for subsequent reads.

        Args:
            filename - name of file corresponding to current example
            cache - `None` or path to cache folder
            i - index of current example (used only in cache filename
                generation). Defaults to -1 (should only be `-1` when `cache` is
                `None`)

        Returns:
            one data point (an output dictionary containing the data comprising
            one example)
        &quot;&quot;&quot;
        if self.cache:
            if self.cache_populated:
                output = self.load_from_cache(i)
            else:
                output = self.populate_cache(filename, i)
        else:
            output = self._generate_example(filename)
            output = self.get_target_length(output, self.current_length)
            
        return output

    def _generate_example(self, filename: str) -&gt; List[Dict[str, Any]]:
        &quot;&quot;&quot;Generates one example (training|validation) from given filename

        Args:
            filename - name of audio file from which to generate example

        Returns:
            one data point (an output dictionary containing the data comprising
            one example)
        &quot;&quot;&quot;
        mix, sources, classes = self.load_audio_files(filename)
        output = self.construct_input_output(mix, sources)
        output[&#39;weights&#39;] = self.get_weights(
            output,
            self.options[&#39;weight_type&#39;]
        )
        output[&#39;log_spectrogram&#39;] = self.whiten(output[&#39;log_spectrogram&#39;])
        output[&#39;classes&#39;] = classes
        output = self.transpose_pad_and_filter(
            output,
            self.options[&#39;length&#39;]
        )
        return output

<div class="viewcode-block" id="BaseDataset.whiten"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.whiten">[docs]</a>    def whiten(self, data):
        &quot;&quot;&quot;Optionally preprocess the data in some way (e.g. make zero mean and unit 
        variance). In Base class, this function does nothing, but it can be overridden
        by a subclass.

        Args:
            data - nparray containing the data

        Returns:
            Processed version of data.
        &quot;&quot;&quot;
        return data</div>

<div class="viewcode-block" id="BaseDataset.construct_input_output"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.construct_input_output">[docs]</a>    def construct_input_output(self, mix, sources):
        &quot;&quot;&quot;Given a mixture signal and source signals, each represented as AudioSignal
        object, construct the input and the output for a neural network. This 
        implementation of this function is capable of computing many different types
        of outputs, depending on the setting in self.options[&#39;output_type&#39;]:

            &#39;msa&#39;: Produces the magnitude spectrum approximation [1]. A network trained
                   with this output will try to match the magnitudes of the ground 
                   truth source when masking the mixture.
            &#39;psa&#39;: Produces the phase-sensitive spectrum approximation [1]. A network
                   trained with this output will try to create a mask of the mixture
                   that takes into account the phase information. This sometimes results
                   in higher separation performance.

        The output of these is stored in dictionary key &#39;source_spectrograms&#39;.
        
        [1] Erdogan, Hakan, John R. Hershey, Shinji Watanabe, and Jonathan Le Roux. 
        &quot;Phase-sensitive and recognition-boosted speech separation using 
        deep recurrent neural networks.&quot; In 2015 IEEE International Conference 
        on Acoustics, Speech and Signal Processing (ICASSP), pp. 708-712. IEEE, 
        2015.

        Args:
            mix - AudioSignal object containing the mixture.
            sources - list of AudioSignal objects containing the sources that sum to
                the mixture.

        Returns:
            A dictionary with four keys: 
                &#39;log_spectrogram&#39;: The log-magnitude spectrogram of the mixture.
                &#39;magnitude_spectrogram&#39;: The magnitude spectrogram of the mixture.
                &#39;source_spectrograms&#39;: A numpy array containing the spectrograms that
                    the separation network should attempt to match. See &#39;psa&#39; and &#39;msa&#39;
                    description above.
                &#39;assignments&#39;: The binary assignments for each time-frequency point. 
                    Useful for training deep clustering networks.
        &quot;&quot;&quot;
        log_spectrogram, mix_stft = self.transform(mix)
        mix_magnitude = np.abs(mix_stft)
        source_magnitudes = []

        for source in sources:
            _, source_stft = self.transform(source)
            source_magnitude = np.abs(source_stft)

            if self.options[&#39;output_type&#39;] == &#39;msa&#39;:
                source_magnitude = np.minimum(mix_magnitude, source_magnitude)
            elif self.options[&#39;output_type&#39;] == &#39;psa&#39;:
                mix_phase = np.angle(mix_stft)
                source_phase = np.angle(source_stft)
                source_magnitude = np.maximum(
                    0.0,
                    np.minimum(
                        mix_magnitude,
                        source_magnitude * np.cos(source_phase - mix_phase),
                    )
                )
            source_magnitudes.append(source_magnitude)

        source_magnitudes = np.stack(source_magnitudes, axis=-1)
        shape = source_magnitudes.shape
        assignments = (
            source_magnitudes == np.amax(source_magnitudes, axis=-1, keepdims=True)
        ).astype(float)

        source_db = librosa.amplitude_to_db(source_magnitudes, ref=np.max)

        output = {
            &#39;log_spectrogram&#39;: log_spectrogram,
            &#39;magnitude_spectrogram&#39;: mix_magnitude,
            &#39;assignments&#39;: assignments,
            &#39;source_spectrograms&#39;: source_magnitudes,
        }

        return output</div>
        
<div class="viewcode-block" id="BaseDataset.transpose_pad_and_filter"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.transpose_pad_and_filter">[docs]</a>    def transpose_pad_and_filter(self, data_dict, max_length):
        &quot;&quot;&quot;For training, each data point should be at least of some length, so that we
        can extract an excerpt that is of &#39;max_length&#39;. The data is also transposed so
        that the length of the sequence is on the first axis and the number of features 
        is on the second axis. This function also filters the data dictionary so that
        keys that are not needed during training are not stored. For a key to be saved
        during training, it should be in self.options[&#39;data_keys_for_training&#39;] 
        when initializing the dataset class. This is to reduce the size of the
        cache (which can get very big if you&#39;re not careful.)

        Args:
            data_dict: dictionary containing keys and values which are numpy arrays.
            max_length: The maximum length of a sequence you use during training.

        Returns:
            Dictionary containing keys and values that are transposed, 
            padded to the max_length, and filtered for unnecessary keys.
        &quot;&quot;&quot;
        length = data_dict[&#39;log_spectrogram&#39;].shape[1]
    
        for i, target in enumerate(self.targets):
            if not np.ndim(data_dict[target]) == 0:
                data = data_dict[target]
                pad_length = max(max_length - length, 0)
                pad_tuple = [(0, 0) for k in range(len(data.shape))]
                pad_tuple[1] = (0, pad_length)
                data_dict[target] = np.pad(data, pad_tuple, mode=&#39;constant&#39;)

        for target in self.targets:
            if self.data_keys_for_training and target not in self.data_keys_for_training:
                data_dict.pop(target)
            else:
                if not np.ndim(data_dict[target]) == 0:
                    data_dict[target] = np.swapaxes(data_dict[target], 0, 1)
        
        return data_dict</div>
    
<div class="viewcode-block" id="BaseDataset.set_current_length"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.set_current_length">[docs]</a>    def set_current_length(self, current_length):
        &quot;&quot;&quot;The current length of sequences can be altered on the fly during training 
        via calls to this function. This is useful for implementing something like
        curriculum learning. You can start with sequences of length 100 and then switch
        to sequences of length 400 during training like so:

            # dset is an initialized subclass of BaseDataset
            dset.set_current_length(100)
            # train for 20 epochs
            dset.set_current_length(400)
            # train at longer length

        Args:
            current_length: length of training sequences.

        Returns:
            No effect. Sets self.current_length = current_length after some validation.
        &quot;&quot;&quot;
        if current_length &gt; self.options[&#39;length&#39;]:
            logging.warning(
                f&quot;current_length={current_length} exceeds original &quot;
                f&quot;set max length {self.options[&#39;length&#39;]}. &quot;
                f&quot;Setting current_length to {self.options[&#39;length&#39;]}&quot;)
            current_length = self.options[&#39;length&#39;]
        self.current_length = current_length</div>

<div class="viewcode-block" id="BaseDataset.get_target_length"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.get_target_length">[docs]</a>    def get_target_length(self, data_dict, target_length):
        &quot;&quot;&quot;Extracts sequences from each key, val pair in data_dict that match a
        given target_length. So if target_length is 400, all of the sequences will be
        400 frames long. There are two strategies for selecting excerpts from the
        training sequences:

            &#39;random&#39;: Randomly select some offset between the start and the length of
                the sequence - the target length.
            &#39;balanced&#39;: First filter all possible offsets to points where both sources
                are actually active. Then select randomly from this list. This is useful
                if you have sparse mixtures where the sources are sometimes overlapping
                in time and sometimes not. This strategy looks for when they are
                overlapping in time and tries to only return those excerpts. This can be
                important for training a separation algorithm.

        Args:
            data_dict: dictionary containing keys and values which are numpy arrays.
            target_length: The length of a sequence you want.
        
        Returns:
            A dictionary with each value trimmed to match the requested target_length.
        &quot;&quot;&quot;
        length = data_dict[&#39;log_spectrogram&#39;].shape[0]
        if self.excerpt_selection_strategy == &#39;random&#39;:
            offset = random.randint(0, length - target_length)
        elif self.excerpt_selection_strategy == &#39;balanced&#39;:
            _balance = data_dict[&#39;assignments&#39;].mean(axis=-3).prod(axis=-1)
            indices = np.argwhere(_balance &gt;= np.percentile(_balance, 50))[:, 0]
            indices[indices &gt; length - target_length] = max(0, length - target_length)
            indices = np.unique(indices)
            offset = random.choice(indices)
                
        for target in data_dict:
            if target != &#39;classes&#39; and not np.ndim(data_dict[target]) == 0:
                data_dict[target] = data_dict[target][
                    offset:offset + target_length,
                    :,
                    :self.options[&#39;num_channels&#39;]
                ]
        return data_dict</div>

<div class="viewcode-block" id="BaseDataset.transform"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.transform">[docs]</a>    def transform(self, audio_signal):
        &quot;&quot;&quot;Uses nussl STFT to transform.

        Args:
            audio_signal {[np.ndarray]} -- AudioSignal object

        Returns:
            [tuple] -- (log_spec, stft). log_spec contains the
            log_spectrogram, stft contains the complex spectrogram
        &quot;&quot;&quot;
        audio_signal.stft_data = None
        stft = (
            audio_signal.stft(use_librosa=self.use_librosa)
        )
        log_spectrogram = librosa.amplitude_to_db(np.abs(stft), ref=np.max)
        return log_spectrogram, stft</div>


<div class="viewcode-block" id="BaseDataset.get_weights"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.get_weights">[docs]</a>    def get_weights(self, data_dict, weight_type):
        &quot;&quot;&quot;During training, the loss for every time-frequency bin can be weighted in
        different ways. There are several weighting schemes that are implemented in
        this function. See the staticmethods below for more information.

        Arguments:
            data_dict: dictionary containing keys and values which are numpy arrays.
            weight_type: a list of weighting schemes that you want. Each set of weights
                is multiplied against the other weights to combine them.
        Returns:
            The square root of the weights. This is to match the weighted 
            version of the deep clustering loss.

        &quot;&quot;&quot;
        weights = np.ones(data_dict[&#39;magnitude_spectrogram&#39;].shape)
        if (&#39;magnitude&#39; in weight_type):
            weights *= self.magnitude_weights(
                data_dict[&#39;magnitude_spectrogram&#39;]
            )
        elif (&#39;source_magnitude&#39; in weight_type):
            weights *= self.source_magnitude_weights(
                data_dict[&#39;source_spectrograms&#39;]
            )
        if (&#39;threshold&#39; in weight_type):
            weights *= self.threshold_weights(
                data_dict[&#39;log_spectrogram&#39;],
                self.options[&#39;weight_threshold&#39;]
            )
        if (&#39;class&#39; in weight_type):
            weights *= self.class_weights(
                data_dict[&#39;assignments&#39;],
            )
        if (&#39;log&#39; in weight_type):
            weights = np.log10(weights + 1)
        return np.sqrt(weights)</div>

<div class="viewcode-block" id="BaseDataset.class_weights"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.class_weights">[docs]</a>    @staticmethod
    def class_weights(assignments):
        &quot;&quot;&quot;More common classes are weighted down while less common classes are weighted
        up.

        Args:
            assignments: returned by construct_input_output, a matrix that indicates the
                 assignment of every time-frequency point in the mixture spectrogram.
        
        Returns:
            Weights based on class frequency.
        &quot;&quot;&quot;
        _shape = assignments.shape 
        assignments = assignments.reshape(-1, _shape[-1])

        class_weights = assignments.sum(axis=0)
        class_weights /= (class_weights.sum() + 1e-7)
        class_weights = 1 / np.sqrt(class_weights + 1e-4)

        weights = assignments @ class_weights 
        weights = weights.reshape(_shape[:-1])
        assignments.reshape(_shape)
        return weights</div>

<div class="viewcode-block" id="BaseDataset.magnitude_weights"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.magnitude_weights">[docs]</a>    @staticmethod
    def magnitude_weights(magnitude_spectrogram):
        &quot;&quot;&quot;Loud points matter more than quiet points.

        Args:
            magnitude_spectrogram: returned by construct_input_output, a matrix indicating
                 magnitude (loudness) of each time-frequency bin.
        
        Returns:
            Weights based on loudness 
        &quot;&quot;&quot;
        weights = magnitude_spectrogram / (np.sum(magnitude_spectrogram) + 1e-6)
        weights *= (
            magnitude_spectrogram.shape[0] * magnitude_spectrogram.shape[1]
        )
        return weights</div>

<div class="viewcode-block" id="BaseDataset.threshold_weights"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.threshold_weights">[docs]</a>    @staticmethod
    def threshold_weights(log_spectrogram, threshold=-40):
        &quot;&quot;&quot;Quiet points don&#39;t matter.

        Args:
            log_spectrogram: returned by construct_input_output, a matrix indicating
                 log magnitude (dB) of each time-frequency bin.
            threshold: loudness threshold. Below this, weights will be 0.
        
        Returns:
            Weights for time-frequency bins below a given threshold are 0. 
        &quot;&quot;&quot;
        return (log_spectrogram &gt; threshold).astype(float)</div>

<div class="viewcode-block" id="BaseDataset.source_magnitude_weights"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.source_magnitude_weights">[docs]</a>    @staticmethod
    def source_magnitude_weights(source_spectrograms):
        &quot;&quot;&quot;Loud points matter more than quiet points, but this time loudness is based
        on each source magnitude.

        Args:
            source_spectrograms: returned by construct_input_output, a matrix with 
                magnitude spectrograms for every source.
        
        Returns:
            Weights based on loudness of each source. 
        &quot;&quot;&quot;
        weights = [
            self.magnitude_weights(source_spectrograms[..., i])
            for i in range(source_spectrograms.shape[-1])
        ]
        weights = np.stack(source_weights, axis=-1)
        weights = source_weights.max(axis=-1)
        return weights</div>

    def _load_audio_file(self, file_path: str) -&gt; AudioSignal:
        &quot;&quot;&quot;Loads audio file at given path. Uses wavfile.read if the extension is a wav
        file as this is considerably faster than using librosa to load the audio file.
        Otherwise, uses the nussl AudioSignal file loader, which just calls 
        librosa.load.

        Args:
            file_path - relative or absolute path to file to load

        Returns:
            AudioSignal object
        &quot;&quot;&quot;
        if os.path.splitext(file_path)[-1] == &#39;.wav&#39;:
            rate, audio = wavfile.read(file_path)
            audio = audio.astype(np.float32, order=&#39;C&#39;) / 32768.0
            audio_signal = AudioSignal(audio_data_array=audio, sample_rate=self.options[&#39;sample_rate&#39;])
        else:
            audio_signal = AudioSignal(file_path, sample_rate=self.options[&#39;sample_rate&#39;])
        audio_signal.path_to_input_file = file_path
        audio_signal.stft_params.window_length = self.options[&#39;n_fft&#39;]
        audio_signal.stft_params.hop_length = self.options[&#39;hop_length&#39;]
        return audio_signal

<div class="viewcode-block" id="BaseDataset.inspect"><a class="viewcode-back" href="../../../../source/src.dataset.html#src.dataset.BaseDataset.inspect">[docs]</a>    def inspect(self, i):
        &quot;&quot;&quot;Useful for inspecting if a dataset implementation is working inside of a 
        Jupyter notebook. It prints every AudioSignal object loaded by load_audio_files,
        visualizes the output of the network, and embeds the audio files via 
        jupyter_utils.embed_audio.

        Args:
            i: Index of the dataset to visualize.
        
        Returns:
            None. Visualizes via plots and audio files.
        &quot;&quot;&quot;
        items = self.load_audio_files(self.files[i])
        output = self.construct_input_output(items[0], items[1])
        output[&#39;weights&#39;] = self.get_weights(
            output,
            self.options[&#39;weight_type&#39;]
        )

        for item in items:
            print(item)
        
        print(&#39;Mixture + original sources&#39;)
        jupyter_utils.embed_audio(items[0])
        for s in items[1]:
            jupyter_utils.embed_audio(s)

        print(&#39;Sources masked from mixture&#39;)
        for j in range(output[&#39;assignments&#39;].shape[-1]):
            mask = SoftMask(output[&#39;assignments&#39;][:, :, :, j])
            items[0].stft()
            signal = items[0].apply_mask(mask)
            signal.istft(overwrite=True, truncate_to_length=items[0].signal_length)
            jupyter_utils.embed_audio(signal)

        plt.subplot(211)
        plt.grid(False)
        plt.imshow(output[&#39;log_spectrogram&#39;].mean(axis=-1), origin=&#39;lower&#39;, aspect=&#39;auto&#39;)
        plt.title(&#39;Mixture spectrogram&#39;)

        plt.subplot(212)
        plt.grid(False)
        plt.imshow(np.argmax(output[&#39;assignments&#39;], axis=-1).mean(axis=-1), origin=&#39;lower&#39;, aspect=&#39;auto&#39;)
        plt.title(&#39;Assignments&#39;)</div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Prem Seetharaman

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>