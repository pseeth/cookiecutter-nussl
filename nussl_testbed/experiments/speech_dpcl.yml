---
dataset_config:
  cache: 'CACHE_DIRECTORY/babywsj8k' # Path is relative to environment variable CACHE_DIRECTORY
  data_keys_for_training:
  - log_spectrogram
  - assignments
  - weights
  excerpt_selection_strategy: random
  format: rnn
  fraction_of_dataset: 1.0
  group_sources: []
  hop_length: 64
  ignore_sources: []
  length: 400
  n_fft: 256
  num_channels: 1
  output_type: psa
  overwrite_cache: false
  sample_rate: 8000
  source_labels: []
  use_librosa_stft: false
  weight_threshold: -40
  weight_type:
  - magnitude

datasets:
  # Three subsets - train/test/val
  # Each subset has a folder and a class.
  # Paths are relative to the environment variable DATA_DIRECTORY
  #   see setup/environment/default.sh for details
  train:
    folder: DATA_DIRECTORY/babywsj/generated/train/
    class: Scaper
  val:
    folder: DATA_DIRECTORY/babywsj/generated/val/
    class: Scaper
  test:
    folder: DATA_DIRECTORY/babywsj/generated/test/
    class: Scaper

info:
  blocking: false
  num_gpus: 1
  project_name: cookiecutter/speech
  experiment_key:
  output_folder: ARTIFACTS_DIRECTORY/cookiecutter/speech/EXPERIMENT_KEY

train_config:
  class: Trainer
  batch_size: 40
  curriculum_learning:
  - args: [400]
    command: set_current_length
    num_epoch: 0
  data_parallel: true
  device: cuda
  initial_length: 400
  learning_rate: 0.0002
  learning_rate_decay: 0.5
  loss_function:
  - !!python/tuple
    - dpcl                # name of loss function
    - embedding       # what output of model to apply the loss function on
    - 1.0             # weight given to the loss function
  num_epochs: 10
  num_workers: 20
  optimizer: adam
  patience: 5
  sample_strategy: sequential
  weight_decay: 0.0
  output_parent_directory: ARTIFACTS_DIRECTORY

model_config:
  class: SeparationModel
  modules:
    log_spectrogram:
      input_shape: [-1, -1, 129]
    normalization:
      class: BatchNorm
      args:
        use_batch_norm: true
    mel_projection:
      class: MelProjection
      args:
        sample_rate: 8000
        num_frequencies: 129
        num_mels: -1
        direction: forward
        trainable: false
        clamp: false
    recurrent_stack:
      class: RecurrentStack
      args:
        num_features: 129
        hidden_size: 300
        num_layers: 4
        bidirectional: true
        dropout: 0.3
        rnn_type: lstm
    embedding:
      class: Embedding
      args:
        num_features: 129
        num_channels: 1
        hidden_size: 600
        embedding_size: 20
        activation:
        - tanh

  connections:
    - !!python/tuple # tuple containing two things:
      - mel_projection # unique name given to module above
      - [log_spectrogram] # list of runtime arguments needed by that module (e.g. output of prev layer)
    - !!python/tuple
      - normalization
      - [mel_projection]
    - !!python/tuple
      - recurrent_stack
      - [normalization]
    - !!python/tuple
      - embedding
      - [recurrent_stack]

  output:
  - embedding

algorithm_config:
  class: DeepClustering
  params:
    clustering_options:
      posterior_alpha: 5.0
    enhancement_amount: 0.0
    mask_type: soft
    num_sources: 2
    percentile: 95

test_config:
  num_workers: 10
  testers:
  - SDR